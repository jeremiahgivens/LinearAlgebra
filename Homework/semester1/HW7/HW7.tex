\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,%
            left=.75in,right=.75in,top=1in,bottom=1in]{geometry}
\setlength{\headsep}{0.25in}

\usepackage{amsthm}

\usepackage{graphicx}
\usepackage{pgfplots}
            
\usepackage[english]{babel}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{case}{Case}

\usepackage{amsthm}
\usepackage{lipsum}
\usepackage{tikz}

\makeatletter
\newcommand{\proofpart}[2]{%
  \par
  \addvspace{\medskipamount}%
  \noindent\emph{Part #1: #2}\par\nobreak
  \addvspace{\smallskipamount}%
  \@afterheading
}
\makeatother

\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\usepackage{mathtools}
\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1 \delimsize\vert #2}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{tkz-euclide}

\DeclareMathOperator{\interior}{int}

\newcommand{\Tau}{\mathcal{T}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\B}{\mathcal{B}}
\DeclarePairedDelimiterX{\iprod}[1]{\langle}{\rangle}{#1}

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}

\usepackage{calligra}
\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n}
\DeclareFontShape{T1}{calligra}{m}{n}{<->s*[2.2]callig15}{}

\newcommand{\scripty}[1]{\ensuremath{\mathcalligra{#1}}}

\pagestyle{fancy}
\author{Jeremiah Givens}
\newcommand{\subject}{Linear Algebra}
\newcommand{\Date}{9/2/2021} 
\makeatletter
\rhead{{\small\@author}}
\lhead{{\small\subject}}
\chead{{\large Homework 6}}
\cfoot{}
\rfoot{\thepage}
\lfoot{\today}

\renewcommand{\theequation}{\arabic{equation}}

\begin{document}
\section*{Problem 1}
\begin{theorem}
Let $T$ be a normal operator on a finite-dimensional inner product space $V$,
and let $W$ be a subspace of $V$. Then $W$ is $T$-invariant if and only if
$W$ is also $T^*$-invariant.
\end{theorem}

\subsection*{Solution}
\begin{proof}
Suppose $W$ is $T$-invariant, and let $\{w_1, \cdots, w_k \}$ be an orthonormal basis for $W$. Extend this to an orthonormal basis of $V$:
\begin{align*}
\beta = \{w_1, \cdots, w_k, v_1, \cdots, v_n \}.
\end{align*}
Then, since $W$ is $T$-invariant, we know $[T]_\beta$ has the form
\begin{align*}
[T]_\beta = \begin{bmatrix}
A && B\\
0 && C
\end{bmatrix}
\end{align*}
where $A$ is an $k \times k$ matrix, and $B$ is an $n \times k$ matrix, and $C$ is an $n \times n$ matrix. With this notation, we have
\begin{align*}
[T^*]_\beta = \begin{bmatrix}
A^* && 0\\
B^* && C^*
\end{bmatrix}.
\end{align*}
Thus, to show that $W$ is $T^*$ invariant, we must show that $B^* = 0$. Now, since $T$ is normal, we have
\begin{align*}
||Tw_i|| = ||T^* w_i|| &\implies ||Tw_i||^2 = ||T^* w_i||^2
\end{align*}
for each $i$. Writing these in terms of our matrices, we see
\begin{align*}
||Tw_i||^2 &= \sum_{j = 1}^k |A_{ji}|^2,
\end{align*}
and
\begin{align*}
||T^*w_i||^2 &= \sum_{j = 1}^k |A^*_{ji}|^2 + \sum_{j = 1}^n |B^*_{ji}|^2\\
&= \sum_{j = 1}^k |\overline{A_{ij}}|^2 + \sum_{j = 1}^n |\overline{B_{ij}}|^2\\
&= \sum_{j = 1}^k |A_{ij}|^2 + \sum_{j = 1}^n |B_{ij}|^2.
\end{align*}
As is, these do not allow for any immediately useful comparison, as the two sums are running over different elements of $A$. To ensure that we run over each element of $A$, we will sum over the square magnitude of the images of each $w_i$. Jumping right in, we see
\begin{align*}
\sum_{i=1}^k ||Tw_i||^2 &= \sum_{i=1}^k \sum_{j = 1}^k |A_{ji}|^2,
\end{align*}
and 
\begin{align*}
\sum_{i=1}^k ||T^*w_i||^2 &= \sum_{i=1}^k\sum_{j = 1}^k |A_{ij}|^2  + \sum_{i=1}^k \sum_{j = 1}^n |B_{ij}|^2 \\
&= \sum_{i=1}^k\sum_{j = 1}^k |A_{ji}|^2  + \sum_{i=1}^k \sum_{j = 1}^n |B_{ij}|^2 \\
&= \sum_{i=1}^k ||Tw_i||^2 + \sum_{i=1}^k \sum_{j = 1}^n |B_{ij}|^2.
\end{align*}
Since the second summation in the equation above must be zero, and each element is nonnegative, it follows that each term in the sum is 0. However, this summation is adding together the complex square of each element of $B^*$, which means we can conclude that $B^* = 0$, as desired.

Now suppose that $W$ is $T^*$-invariant. Since $T^{**} = T$, and $T^*$ is normal, it follows immediately from the section above that $W$ is $T$-invariant.
\end{proof}

\section*{Problem 2}
A linear operator $T$ on a finite-dimensional inner product space over $\R$
or $\C$ is called \textit{positive definite} (resp. \textit{positive semi-
definite}) if $T$ is self-adjoint and $\langle T(x), x \rangle > 0$ (resp. $\langle
T(X), x \rangle \geq 0$) for all $x \neq 0$. A matrix $A$ is called positive
(semi)definite if $L_A$ is positive (semi)definite.

\begin{theorem}
Let $T$ be hermitian. Then, the following are true
\proofpart{(a)}{} $T$ is positive (semi-)definite if and only if all of its
eigenvalues are $>0$ ($\geq 0$).
\proofpart{(b)}{} $T$ is positive semi-definite if and only if there is an
operator $U$ such that $T = U^* U$.
\end{theorem}

\subsection*{Solution}
\begin{proof}
\proofpart{(a)}{}
Suppose $T$ is positive (semi-)definite, and let $v$ be an eigenvector of $T$ with corresponding eigenvalue $\lambda$. Then, we have
\begin{align*}
0 &> (\geq) \iprod{Tv, v}\\
&= \iprod{\lambda v, v}\\
&= \lambda \iprod{v, v}.
\end{align*}
Since $\iprod{v, v} > 0$, we can conclude that $\lambda > (\geq) 0$.

Now suppose all of $T$s eigenvalues are $>0$ ($\geq 0$). Since $T$ is self-adjoint, there exists an orthonormal basis of $V$ consisting of the eigenvectors of $T$,
\begin{align*}
\beta = \{v_1, ..., v_n\}
\end{align*}
with corresponding eigenvalues $\lambda_i$. Let $x \in V$ be nonzero. Then, there exist $a_i \in \mathbb{F}$ (atleast one non-zero) such that 
\begin{align*}
x = \sum a_i v_i.
\end{align*}
Then, we have
\begin{align*}
\iprod{Tx, x} &= \iprod{T \left(\sum a_i v_i \right), \sum a_i v_i}\\
&= \iprod{\sum \lambda_i a_i v_i, \sum a_i v_i}\\
&= \sum \lambda_i a_i \iprod{v_i, \sum a_i v_i}\\
&= \sum \lambda_i a_i \overline{a_i}\\
&= \sum \lambda_i |a_i|^2.
\end{align*}
Since each of these terms is nonnegative, and at least one $a_i$ is nonzero, we have $\langle T(x), x \rangle > 0$ (resp. $\langle T(X), x \rangle \geq 0$), and our proof of (a) is complete.

\proofpart{(b)}{}
Suppose $T$ is positive semi-definite. Let $T_i$ and $\lambda_i$ and $k$ be defined as in the spectral theorem. Define an operator $U$ on $V$ by 
\begin{align*}
U = \sum_{i=1}^k \sqrt{\lambda_i} T_i.
\end{align*}
From part(a) of this theorem, we have that $U$ is well defined, as each of the eigenvalues is a non-negative real number. Then, we have 
\begin{align*}
U^* &= \left(\sum_{i=1}^k \sqrt{\lambda_i} T_i \right)^*\\
&= \sum_{i=1}^k \overline{\sqrt{\lambda_i}} T^*_i\\
&= \sum_{i=1}^k \sqrt{\lambda_i} T_i &&\text{Orthogonal projections are hermitian}.
\end{align*}
Thus,
\begin{align*}
U^*U &= \sum_{i=1}^k \lambda_i T_i &&\text{By part (c) of the spectral theorem}\\
&= T &&\text{By part (e) of the spectral theorem.}
\end{align*}

Now suppose that $T = U^* U$ for some operator $U$ on $V$. Then, for any $x \in V$, we have
\begin{align*}
\iprod{Tx, x} &= \iprod{U^*U x, x}\\
&= \iprod{U x, U^* x}\\
&\geq 0,
\end{align*}
and we can conclude that $T$ is self-adjoint.
\end{proof}

\section*{Problem 3}
\begin{theorem}
Let $T$ be a normal operator on a complex finite dimensional inner product space $V$, and let $U$ be any operator on $V$. If the
eigenspaces of $T$ are $U$-invariant, $TU = UT$.
\end{theorem}

\subsection*{Solution}
\begin{proof}
Let $W_i, \lambda_i$, and $k$ be defined as in the spectral theorem, and let $x \in V$. By the spectral theorem, there exist $x_i \in W_i$ such that
\begin{align*}
x = \sum_{i=1}^k x_i.
\end{align*}
Then, we have 
\begin{align*}
UTx &= UT \sum_{i=1}^k x_i\\
&= \sum_{i=1}^k UTx_i\\
&= \sum_{i=1}^k U \lambda_i x_i &&x_i \in W_i\\
&= \sum_{i=1}^k \lambda_i U x_i\\
&= \sum_{i=1}^k TUx_i && Ux_i \in W_i\\
&= TU \sum_{i=1}^k x_i\\
&= TUx
\end{align*}
Since this is true for all $x \in V$, we have shown that $TU = UT$.
\end{proof}

\section*{Problem 4}
Suppose that $T$ is normal on the finite dimensional complex inner product
space. Use the spectral decomposition of $T$,
\[T = \lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k,\]
to prove the following results.
\begin{enumerate}
\item $T$ is invertible if and only if $\lambda_i \neq 0$ for $1 \leq i \leq
k$.
\item $T$ is a projection if and only if every eigenvalue of $T$ is $1$ or $0$.
\item $T = - T^*$ if and only if every eigenvalue $\lambda_i$ is imaginary.
\end{enumerate}

\subsection*{Solution}
\begin{proof}
\begin{enumerate}
\item For the forward direction, we will prove the contrapositive. Suppose that there exists an $i$ such that $\lambda_i = 0$. Let $W_j$ be the eigenspace of $T$ corresponding to $\lambda_i$. Then, if $x_i \in W_i$, we have
\begin{align*}
Tx_i &= \lambda_1 T_1 x_i + \cdots + \lambda_i T_i x_i + \cdots + \lambda_k T_k x_i\\
&= \lambda_i x_i && T_j \text{ are orthogonal projections}\\
&= 0.
\end{align*}
Thus, $x_i$ is a non-zero vector in the nullspace of $T$, which means $T$ is non-invertible, proving the contrapositive.

For the other direction, suppose every eigenvalue of $T$ is nonzero, and let $x \in V$ be nonzero. By the spectral theorem, there exist $x_i \in W_i$, not all zero, such that
\begin{align*}
x = x_1 + \cdots + x_k.
\end{align*}
Then, 
\begin{align*}
Tx &= \lambda_1 T_1 x  + \cdots + \lambda_k T_k x\\
&= \lambda_1 x_1 + \cdots + \lambda_k x_k.
\end{align*}
Thus, since any of these vectors which are nonzero are linearly independent (and we know at least one is nonzero), and the eigenvalues are nonzero, we can conclude that $Tx \not = 0$. Since this is true for any nonzero $x$, we can conclude that the only vector in $T$s nullspace is the zero vector, implying that $T$ is invertible.

\item Using part (c) of the spectral theorem, we have
\begin{align*}
T \text{ is a projection} &\iff T^2 = T \\
&\iff (\lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k)(\lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k) = \lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k\\
&\iff \lambda^2_1 T_1 + \lambda^2_2 T_2 + \cdots + \lambda^2_k T_k = \lambda_1 T_1 + \lambda_2 T_2 + \cdots + \lambda_k T_k\\
&\iff (\forall 1 \leq i \leq k)(\lambda_i = 1) \lor (\forall 1 \leq i \leq k)(\lambda_i = 0),
\end{align*}
where the last line follows from the fact that these orthogonal operators are linearly independent in the vector space of linear operators on $V$. To see that these are in fact linearly independent, let $a_i$ be such that 
\begin{align*}
\sum_i^k a_iT_i = 0.
\end{align*}
Letting $x_i \in W_i$ be nonzero, we see
\begin{align*}
0 &= 0x_i\\
&= \sum_i^k a_iT_i x_i\\
&= a_i x_i,
\end{align*}
and we can conclude that $a_i = 0$ for all $i$, as desired.

\item Let $\lambda_j = a_j + ib_j$. Then,
\begin{align*}
T &= \lambda_1 T_1  + \cdots + \lambda_k T_k\\
&= (a_1 + ib_1) T_1  + \cdots + (a_k + ib_k) T_k\\
\end{align*}
and
\begin{align*}
-T^* &= -(\lambda_1 T_1  + \cdots + \lambda_k T_k)^*\\
&= -(\overline{\lambda_1} T_1  + \cdots + \overline{\lambda_k} T_k) && \text{Orthogonal projections are hermitian}\\
&= -(\overline{(a_1 + ib_1)} T_1  + \cdots + \overline{(a_k + ib_k)} T_k)\\
&= -((a_1 - ib_1) T_1  + \cdots + (a_k - ib_k) T_k)\\
&= (-a_1 + ib_1) T_1  + \cdots + (-a_k + ib_k) T_k.
\end{align*}
Thus, by linear independence of each $T_k$, we have 
\begin{align*}
T = -T^* &\iff (\forall 1 \leq i \leq k)(a_i = 0)\\
&\iff (\forall 1 \leq i \leq k)(\lambda_i \text{ is imaginary})
\end{align*}
\end{enumerate}
\end{proof}

\end{document}
